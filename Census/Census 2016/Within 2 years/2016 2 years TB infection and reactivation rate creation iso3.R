#'===================================================================================================================#
#'
#' This script takes annual risk of infection (ARI) values developed by Pete Dodd and Rein Houben and 
#' generates force of infection (FOI) estimates for differing population groups (by age, year of arrival 
#' and country of birth), which can then be converted to probabilities/risk of infection and combined 
#' with census datasets to arrive at prevalence of TB infection estimates for a particular setting.
#' The script will  calculate the median hazard for each population group in the census and, additionally, will provide lower and 
#' upper percentile estimates, as defined by the following
#' objects.
#' These are also combined with TB data to provide estimates of reactivation rates/risks.
#'
#' INPUTS:
#'  - ARTI estimates generated by Pete Dodd and Rein Houben ("5000repLARI.Rdata" and "200repLARI.Rdata")
#'  - Census data by age, year of arrival, and country of birth. This file needs to be cleaned
#'  and reshaped, if necessary, with the "CleanseCensus" function to create a dataset with the
#'  following columns:
#'      -  NUMP - Number of persons in the population group
#'      -  AGEP - Age of population group
#'      -  YARP - Year of arrival of population group
#'      -  YOBP - Year of birth of population group
#'      -  BPLP - Birth country of population group
#'      -  ISO3 - 3 letter country code corresponding to birth country 
#'      -  CNSY - Census year
#'  - TB data by year of notification, age, year of arrival, and country of birth. This file needs to be cleaned
#'  and reshaped, if necessary, with the "CleanseTBdata" function to create a dataset with the
#'  following columns:
#'      -  year - Year of notification
#'      -  poptb - Number of TB cases in the population group
#'      -  AGEP - Age of population group
#'      -  YARP - Year of arrival of population group
#'      -  ISO3 - 3 letter country code corresponding to birth country 
#' 
#' OUTPUTS:
#' - A master table of all TB infection probabilities and outcomes, 
#' stored in the outputs path specified below (path.out).
#'      -  NUMP - Number of persons in the population group
#'      -  AGEP - Age of population group
#'      -  YARP - Year of arrival of population group
#'      -  YOBP - Year of birth of population group
#'      -  BPLP - Birth country of population group
#'      -  ISO3 - 3 letter country code corresponding to birth country 
#'      -  CNSY - Census year
#'      -  LTBP - Number of in the population estimated to have been infected
#'      -  PROB - Probability/risk of infection in the population group
#' 
#' Author: Katie Dale and Milinda Abayawardana
#' Date created: 2016-2021
#'===================================================================================================================#

rm(list = ls(all.names = TRUE)) # Clears all objects includes hidden objects.
gc() # Frees up memory and report the memory usage.

#' LOAD REQUIRED LIBRARIES ==========================================================================================#

library(data.table)
library(tidyverse)
library(reshape2)
library(countrycode)

#' DEFINE USEFUL OBJECTS (INPUT & OUTPUT PATHS, FUNCTIONS, STUDY PERIOD ETC) ========================================#

#' The iso3 value for the country from which the census data is from

census.iso3 <- "CAN"

census.year <- 2016

#' This script will calculate the median hazard for each population
#' group in the census and, additionally, will provide lower and 
#' upper percentile estimates, as defined by the following
#' objects.
low.percentile <- 0.025
high.percentile <- 0.975

#' Data inputs file path
path.in <- "TBI-Prevalence-Canada/Census/"

#' Output file path
path.out <- "TBI-Prevalence-Canada/Analysis and Outputs/"

#' Source all functions, which are located within the "Functions" file.
source(paste0(path.in, "Functions3 iso3.R"))

#' INPUTS ===========================================================================================================#

#' Load the hazard data from Houben & Dodd
# filename <- paste0(path.in, "5000repLARI.Rdata")
# 
# tbhaz.5000rep <- load(filename)
# tbhaz.5000rep <- LARI
# rm(LARI)

filename <- paste0(path.in, "200repLARI.Rdata")
load(filename)
tbhaz.200rep <- as.data.table(rundata)
rm(rundata)
View(tbhaz.200rep)

# tbhaz.200rep <- tbhaz.200rep%>%
#   mutate(lari = ifelse(year==2014, lari, NA))%>%
#   arrange(iso3, replicate)
# 
# tbhaz.200rep$lari <- na.locf(tbhaz.200rep$lari, fromLast = T)

#View(tbhaz.200rep)
#' Load the census data 
census <- read.csv(paste0(path.in, "2016_census_classified.csv"), header = T)
#View(census)
census <- census%>%select(-AGE_GROUP, -YARP_group)
colnames(census) <- c("ISO3", "WHO_R","AGEP","YARP","BPLP","CNSY","YOBP","NUMP")

# Remove ISO3s that are not in Houben and Dodd dataset or they will give a result of 0 LTBP while including the population. This will lead to an underestimate of prevalence

#census <- census%>%filter(!ISO3%in%c("FRO", "GGY", "GIB", "GLP", "GUF", "IMN", "JEY", "MTQ", "SPM", "TWN"))

census <- census%>%filter(!ISO3%in%c("FLK", "GIB", "GLP", "GUF", "LIE", "MTQ"))

#View(census)
# census <- read.csv(paste0(path.in, "Australia 2006.csv"), skip = 9, header = T)
# census <- read.csv(paste0(path.in, "Australia 2011.csv"), skip = 9, header = T)
#View(census)
#' Load TB data 
tb <- read.csv(paste0(path.in, "NNDSS skeleton.csv"), header = T)

#' DATA PREP ========================================================================================================#

#' Creating cumulative FOIs and adjusting for census year.
#' Also expanding the table's year range ( 1889 to 2016).
tbhaz.200rep <- tbhazprep.function(tbhaz.200rep)

#View(tbhaz.200rep)
census_collapsed <- census%>%
  group_by(ISO3, YARP)%>%
  summarise(NUMP_yarp = sum(NUMP))

# FOI STRAIGHT TO ARI

tbhaz.200rep_merger <- tbhaz.200rep%>%
  filter(year%in%c(2015,2016))%>%
  select(year, iso3, replicate, FOI)%>%
  rename(ISO3 = iso3)

#View(tbhaz.200rep_merger)

census_and_haz <- left_join(tbhaz.200rep_merger, census_collapsed, by = "ISO3")
View(census_and_haz)

prob_can <- unique(census_and_haz%>%
  filter(ISO3=="CAN" & year=="2016")%>%
  select(replicate, FOI)%>%
  mutate(PROB_CAN = 2*(1-exp(-FOI)))%>%
  select(-FOI))
#View(prob_can)

census_and_haz_withCAN <- left_join(census_and_haz, prob_can, by = "replicate")

census_probs <- census_and_haz_withCAN%>%
           mutate(PROB = ifelse(YARP ==2015, (1-exp(-FOI)+PROB_CAN),
                                ifelse(YARP == "2016", 2*(1-exp(-FOI)),
                                PROB_CAN)))%>%
  mutate(LTBP = PROB*NUMP_yarp)

View(census_probs)

census_replicates <- census_probs%>%
  filter(ISO3!="CAN")%>%
  group_by(ISO3, replicate)%>%
  summarise(LTBP_replicates = sum(LTBP),
    NUMP_replicates = sum(NUMP_yarp))

View(census_replicates)


# Exclude all that aren't the 168
library(readxl)
exclusion_table <- read_xlsx("/Users/ajordan/Library/CloudStorage/OneDrive-McGillUniversity/A TB/Project Writing/TBI Prev Pub/exclusion_table.xlsx")

exclusion_vector <- exclusion_table$ISO3...1

tb_inc <- readRDS("/Users/ajordan/Library/CloudStorage/OneDrive-McGillUniversity/LTBI-Aust-CEA-master/Data/incidence.rds")

colnames(tb_inc) <- c("ISO3", "WHO_Region", "e_inc_100k")
tb_inc <- tb_inc%>%select(-WHO_Region)
head(tb_inc)

tbi_data <- merge(census_replicates, tb_inc, by = "ISO3")

tbi_prev_aarp_2years <- tbi_data%>%
  filter(NUMP_replicates > 0 & !is.na(NUMP_replicates))%>%
  filter(ISO3!="CAN")%>%
  filter(!ISO3%in%c(exclusion_vector))%>%
  mutate(inc_band_per100k = ifelse(
    e_inc_100k < 10, "0-9", ifelse(
      e_inc_100k >= 10 & e_inc_100k < 50, "10-49", ifelse(
        e_inc_100k >= 50 & e_inc_100k < 100, "50-99", ifelse(
          e_inc_100k >= 100 & e_inc_100k < 200, "100-199",ifelse(
            e_inc_100k >= 200 & e_inc_100k <= 1000, "200+", "No value"))))))%>%
  group_by(inc_band_per100k, replicate)%>%
  summarise(LTBP_sum =sum(LTBP_replicates),
            NUMP_sum = sum(NUMP_replicates))%>%
  #group_by(inc_band_per100k)%>%
  summarise(tbi_prev_median = median(LTBP_sum)/NUMP_sum,
            tbi_prev_low = quantile(LTBP_sum, probs = 0.025)/NUMP_sum,
            tbi_prev_high = quantile(LTBP_sum, probs = 0.975)/NUMP_sum)

View(unique(tbi_prev_aarp_2years))

tbi_data%>%
  filter(NUMP_replicates > 0 & !is.na(NUMP_replicates))%>%
  filter(ISO3!="CAN")%>%
  filter(!ISO3%in%c(exclusion_vector))%>%
  group_by(replicate)%>%
  summarise(LTBP_sum =sum(LTBP_replicates),
            NUMP_sum = sum(NUMP_replicates))%>%
  #group_by(inc_band_per100k)%>%
  summarise(tbi_prev_median = median(LTBP_sum)/NUMP_sum,
            tbi_prev_low = quantile(LTBP_sum, probs = 0.025)/NUMP_sum,
            tbi_prev_high = quantile(LTBP_sum, probs = 0.975)/NUMP_sum)


# IMMIGRATION WITHIN LAST TWO YEARS

census_probs_immigration <- census_and_haz_withCAN%>%
  filter(YARP >= 2015)%>%
  mutate(PROB = ifelse(YARP ==2015, (1-exp(-FOI)+PROB_CAN),
                       ifelse(YARP == "2016", 2*(1-exp(-FOI)),
                              PROB_CAN)))%>%
  mutate(LTBP = PROB*NUMP_yarp)

View(census_probs_immigration)

census_replicates_immigration <- census_probs_immigration%>%
  filter(ISO3!="CAN")%>%
  group_by(ISO3, replicate)%>%
  summarise(LTBP_replicates = sum(LTBP),
            NUMP_replicates = sum(NUMP_yarp))

#View(census_replicates_immigration)

# Exclude all that aren't the 168

tbi_data_immigration <- merge(census_replicates_immigration, tb_inc, by = "ISO3")

# Estimate with or without TB disease in country of origin
tbi_prev_aarp_2years_immigration <- tbi_data_immigration%>%
  filter(NUMP_replicates > 0 & !is.na(NUMP_replicates))%>%
  filter(ISO3!="CAN")%>%
  filter(!ISO3%in%c(exclusion_vector))%>%
  mutate(inc_band_per100k = ifelse(
    e_inc_100k < 10, "0-9", ifelse(
      e_inc_100k >= 10 & e_inc_100k < 50, "10-49", ifelse(
        e_inc_100k >= 50 & e_inc_100k < 100, "50-99", ifelse(
          e_inc_100k >= 100 & e_inc_100k < 200, "100-199",ifelse(
            e_inc_100k >= 200 & e_inc_100k <= 1000, "200+", "No value"))))))%>%
  group_by(inc_band_per100k, replicate)%>%
  summarise(LTBP_sum =sum(LTBP_replicates),
            NUMP_sum = sum(NUMP_replicates))%>%
  #group_by(inc_band_per100k)%>%
  summarise(tbi_prev_median = median(LTBP_sum)/NUMP_sum,
            tbi_prev_low = quantile(LTBP_sum, probs = 0.025)/NUMP_sum,
            tbi_prev_high = quantile(LTBP_sum, probs = 0.975)/NUMP_sum)

View(unique(tbi_prev_aarp_2years_immigration))